{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df=pd.read_csv(\"data/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df=pd.read_csv(\"data/updated_movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_movie_df=movies_df[movies_df[\"genres\"]!=\"UnKnown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_movie_df=samples_movie_df[((samples_movie_df['avg_user_rating']>=4)& \n",
    "                                   (samples_movie_df['year']>=1980))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ratings_df=ratings_df.merge(samples_movie_df,on=\"movieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId             2506487\n",
       "movieId            2506487\n",
       "rating             2506487\n",
       "timestamp          2506487\n",
       "title              2506487\n",
       "genres             2506487\n",
       "avg_user_rating    2506487\n",
       "year               2506487\n",
       "comb               2506487\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ratings_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ratings_df.to_csv(\"data/sample_ratings_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "scSpark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"6g\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into spark dataframe\n",
    "ratings = scSpark.read\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .csv(\"data/sample_ratings_df.csv\", header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- avg_user_rating: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- comb: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check dataframe schema\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+------+----------+-------------------+---------------------------+-----------------+----+---------------------------------------+\n",
      "|_c0|userId|movieId|rating|timestamp |title              |genres                     |avg_user_rating  |year|comb                                   |\n",
      "+---+------+-------+------+----------+-------------------+---------------------------+-----------------+----+---------------------------------------+\n",
      "|0  |1     |296    |5.0   |1147880044|Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|1  |3     |296    |5.0   |1439474476|Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|2  |4     |296    |4.0   |1573938898|Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|3  |5     |296    |4.0   |830786155 |Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|4  |7     |296    |4.0   |835444730 |Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|5  |8     |296    |5.0   |890489713 |Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|6  |10    |296    |4.5   |1227571308|Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|7  |12    |296    |5.0   |1119354604|Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|8  |13    |296    |5.0   |1238029599|Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|9  |14    |296    |5.0   |1506208897|Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|10 |15    |296    |5.0   |946013314 |Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|11 |18    |296    |3.0   |1108273426|Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|12 |19    |296    |3.5   |1466674637|Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|13 |22    |296    |3.0   |832862128 |Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|14 |23    |296    |5.0   |942964550 |Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|15 |25    |296    |5.0   |836216901 |Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|16 |26    |296    |3.0   |1280515788|Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|17 |31    |296    |3.0   |1256229864|Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|18 |32    |296    |5.0   |839020162 |Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "|19 |33    |296    |4.5   |1552804447|Pulp Fiction (1994)|Comedy Crime Drama Thriller|4.188912039361382|1994|Comedy Crime Drama Thriller 4.1889 1994|\n",
      "+---+------+-------+------+----------+-------------------+---------------------------+-----------------+----+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check for first 20 records\n",
    "ratings.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop timestamp\n",
    "ratings = ratings.drop(*['timestamp','avg_user_rating','year','comb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+------+-------------------+--------------------+\n",
      "|_c0|userId|movieId|rating|              title|              genres|\n",
      "+---+------+-------+------+-------------------+--------------------+\n",
      "|  0|     1|    296|   5.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "|  1|     3|    296|   5.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "|  2|     4|    296|   4.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "|  3|     5|    296|   4.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "|  4|     7|    296|   4.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "|  5|     8|    296|   5.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "|  6|    10|    296|   4.5|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "|  7|    12|    296|   5.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "|  8|    13|    296|   5.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "|  9|    14|    296|   5.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "| 10|    15|    296|   5.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "| 11|    18|    296|   3.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "| 12|    19|    296|   3.5|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "| 13|    22|    296|   3.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "| 14|    23|    296|   5.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "| 15|    25|    296|   5.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "| 16|    26|    296|   3.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "| 17|    31|    296|   3.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "| 18|    32|    296|   5.0|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "| 19|    33|    296|   4.5|Pulp Fiction (1994)|Comedy Crime Dram...|\n",
      "+---+------+-------+------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2506487"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the number of records in dataframe\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o17367.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 4419.0 failed 1 times, most recent failure: Lost task 7.0 in stage 4419.0 (TID 50411, DESKTOP-LEV8C79, executor driver): java.io.FileNotFoundException: C:\\Users\\vipul\\AppData\\Local\\Temp\\blockmgr-3afa0c21-1a0b-4a04-b4f9-d5950e1a0b3b\\23\\temp_shuffle_2abcc70f-0573-4bd3-99d2-d398b5ab2cb9 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:105)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:118)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:245)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2979)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2978)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2978)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\vipul\\AppData\\Local\\Temp\\blockmgr-3afa0c21-1a0b-4a04-b4f9-d5950e1a0b3b\\23\\temp_shuffle_2abcc70f-0573-4bd3-99d2-d398b5ab2cb9 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:105)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:118)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:245)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-c4af1efa4b3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Reducing the size of dataset initially to train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msample_ratings_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msample_ratings_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \"\"\"\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o17367.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 4419.0 failed 1 times, most recent failure: Lost task 7.0 in stage 4419.0 (TID 50411, DESKTOP-LEV8C79, executor driver): java.io.FileNotFoundException: C:\\Users\\vipul\\AppData\\Local\\Temp\\blockmgr-3afa0c21-1a0b-4a04-b4f9-d5950e1a0b3b\\23\\temp_shuffle_2abcc70f-0573-4bd3-99d2-d398b5ab2cb9 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:105)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:118)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:245)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2979)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2978)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2978)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\vipul\\AppData\\Local\\Temp\\blockmgr-3afa0c21-1a0b-4a04-b4f9-d5950e1a0b3b\\23\\temp_shuffle_2abcc70f-0573-4bd3-99d2-d398b5ab2cb9 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:105)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:118)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:245)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "#Reducing the size of dataset initially to train model\n",
    "sample_ratings_df = ratings.sample(fraction=0.02, seed=42)\n",
    "sample_ratings_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ratings_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"spark_lens\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register temp table.\n",
    "#checking for nulls \n",
    "sample_ratings_df.registerTempTable('rating')\n",
    "# Query table for number of nulls.\n",
    "spark.sql('''\n",
    "    SELECT COUNT(rating) AS nulls\n",
    "    FROM rating \n",
    "    WHERE rating=null\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Training and Test data\n",
    "training, test = sample_ratings_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Basic Model\n",
    "als = ALS(nonnegative=True, implicitPrefs=False, coldStartStrategy=\"drop\")\\\n",
    ".setMaxIter(5)\\\n",
    ".setRegParam(0.01)\\\n",
    ".setUserCol(\"userId\")\\\n",
    ".setItemCol(\"movieId\")\\\n",
    ".setRatingCol(\"rating\")\\\n",
    "\n",
    "\n",
    "# Confirm that a model called \"als\" was created\n",
    "type(als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als.explainParams()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the requisite items\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Add hyperparameters and their respective values to param_grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 50, 100, 150]) \\\n",
    "            .addGrid(als.regParam, [.01, .05, .1, .15]) \\\n",
    "            .build()\n",
    "           \n",
    "# Define evaluator as RMSE and print length of evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\") \n",
    "print (\"Num models to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cross validation using CrossValidator\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Confirm cv was built\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit cross validator to the 'train' dataset\n",
    "model = cv.fit(training)\n",
    "\n",
    "#Extract best model from the cv model above\n",
    "alsmodel = model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.recommendation.ALSModel'>\n",
      "**Best Model**\n",
      "  Rank: 10\n",
      "  MaxIter: 5\n",
      "  RegParam: 0.15\n"
     ]
    }
   ],
   "source": [
    "# Print best_model\n",
    "print(type(alsmodel))\n",
    "\n",
    "# Complete the code below to extract the ALS model parameters\n",
    "print(\"**Best Model**\")\n",
    "\n",
    "# # Print \"Rank\"\n",
    "print(\"  Rank:\", alsmodel._java_obj.parent().getRank())\n",
    "\n",
    "# Print \"MaxIter\"\n",
    "print(\"  MaxIter:\", alsmodel._java_obj.parent().getMaxIter())\n",
    "\n",
    "# Print \"RegParam\"\n",
    "print(\"  RegParam:\", alsmodel._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6255027249120588\n"
     ]
    }
   ],
   "source": [
    "# View the predictions\n",
    "test_predictions = alsmodel.transform(test)\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+------+--------------------+--------------------+-----------------+----+--------------------+----------+\n",
      "|    _c0|userId|movieId|rating|               title|              genres|  avg_user_rating|year|                comb|prediction|\n",
      "+-------+------+-------+------+--------------------+--------------------+-----------------+----+--------------------+----------+\n",
      "|1597692| 29595|  48780|   5.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 2.5641563|\n",
      "|1598476| 35094|  48780|   4.5|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 2.6400785|\n",
      "|1607676|100007|  48780|   4.5|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 2.3039634|\n",
      "|1613577|142494|  48780|   4.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 2.0484178|\n",
      "|1600828| 51589|  48780|   4.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 2.4633114|\n",
      "|1614658|150005|  48780|   4.5|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...|  1.252698|\n",
      "|1614034|145678|  48780|   4.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...|  3.693248|\n",
      "|1609436|112680|  48780|   5.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 3.1682866|\n",
      "|1607146| 96234|  48780|   4.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 2.3303924|\n",
      "|1607298| 97333|  48780|   4.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 2.5201175|\n",
      "|1605774| 86527|  48780|   3.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 3.9505277|\n",
      "|1603384| 69523|  48780|   4.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 2.1166992|\n",
      "|1605904| 87441|  48780|   4.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 3.2530897|\n",
      "|1593516|   429|  48780|   4.5|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 2.3563728|\n",
      "|1615326|154893|  48780|   5.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 2.7097275|\n",
      "|1599003| 38794|  48780|   4.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 3.2850938|\n",
      "|1599165| 39912|  48780|   4.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 2.2029448|\n",
      "|1608534|106016|  48780|   4.5|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 2.5599592|\n",
      "|1609296|111563|  48780|   4.0|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...|  3.537697|\n",
      "|1615153|153670|  48780|   2.5|Prestige, The (2006)|Drama Mystery Sci...|4.093231050865188|2006|Drama Mystery Sci...| 3.5424957|\n",
      "+-------+------+-------+------+--------------------+--------------------+-----------------+----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|  1342|[[171773, 5.80972...|\n",
      "|  1959|[[165575, 9.34316...|\n",
      "|  2142|[[165575, 5.14053...|\n",
      "|  3175|[[166291, 5.61728...|\n",
      "|  3794|[[132253, 8.81997...|\n",
      "|  4935|[[172197, 7.07680...|\n",
      "|  5300|[[165575, 6.81612...|\n",
      "|  5803|[[158894, 9.78420...|\n",
      "|  6336|[[165575, 6.74957...|\n",
      "|  6658|[[116975, 9.19224...|\n",
      "|  7240|[[171773, 5.17630...|\n",
      "|  7340|[[165575, 5.09727...|\n",
      "|  7880|[[165575, 5.14053...|\n",
      "|  7993|[[171773, 5.80972...|\n",
      "|  9852|[[175397, 6.13809...|\n",
      "| 10362|[[170683, 5.64290...|\n",
      "| 10623|[[171773, 8.29960...|\n",
      "| 11141|[[150696, 6.46160...|\n",
      "| 15447|[[165575, 7.83186...|\n",
      "| 15619|[[165575, 5.81849...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nrecoomendations=alsmodel.recommendForAllUsers(5)\n",
    "nrecoomendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+\n",
      "|userId|movieId|   rating|\n",
      "+------+-------+---------+\n",
      "|  1342| 171773| 5.809721|\n",
      "|  1342| 165575|5.7393336|\n",
      "|  1342| 148667|5.7349095|\n",
      "|  1342| 116975| 5.635259|\n",
      "|  1342| 168716|  5.58637|\n",
      "|  1959| 165575| 9.343166|\n",
      "|  1959| 170683| 8.687488|\n",
      "|  1959| 196631| 8.687488|\n",
      "|  1959| 111562| 8.687488|\n",
      "|  1959| 145060| 8.687488|\n",
      "+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nrecoomendations = nrecoomendations\\\n",
    "    .withColumn(\"rec_exp\", explode(\"recommendations\"))\\\n",
    "    .select('userId', col(\"rec_exp.movieId\"), col(\"rec_exp.rating\"))\n",
    "\n",
    "nrecoomendations.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into spark dataframe\n",
    "movies = scSpark.read\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .csv(\"data/movies.csv\", header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+------+--------------------+--------------------+------------------+----+--------------------+--------------------+--------------------+\n",
      "|movieId|    _c0|userId|rating|               title|              genres|   avg_user_rating|year|                comb|               title|              genres|\n",
      "+-------+-------+------+------+--------------------+--------------------+------------------+----+--------------------+--------------------+--------------------+\n",
      "|   4973|  97826|  7340|   5.0|Amelie (Fabuleux ...|      Comedy Romance| 4.101282051282051|2001|Comedy Romance 4....|Amelie (Fabuleux ...|      Comedy|Romance|\n",
      "|   6016| 182780|  7340|   5.0|City of God (Cida...|Action Adventure ...|  4.18158741329044|2002|Action Adventure ...|City of God (Cida...|Action|Adventure|...|\n",
      "|    318| 297826|  7340|   5.0|Shawshank Redempt...|         Crime Drama| 4.413576004516335|1994|Crime Drama 4.413...|Shawshank Redempt...|         Crime|Drama|\n",
      "|  44555|2163096|  7340|   5.0|Lives of Others, ...|Drama Romance Thr...| 4.200392285060477|2006|Drama Romance Thr...|Lives of Others, ...|Drama|Romance|Thr...|\n",
      "|   2931|2405328|  7340|   5.0|Time of the Gypsi...|Comedy Crime Dram...|4.0220750551876385|1989|Comedy Crime Dram...|Time of the Gypsi...|Comedy|Crime|Dram...|\n",
      "|     50| 973471|  7340|   4.5|Usual Suspects, T...|Crime Mystery Thr...| 4.284353213163313|1995|Crime Mystery Thr...|Usual Suspects, T...|Crime|Mystery|Thr...|\n",
      "|    608|1968521|  7340|   4.5|        Fargo (1996)|Comedy Crime Dram...|4.1114212826464245|1996|Comedy Crime Dram...|        Fargo (1996)|Comedy|Crime|Dram...|\n",
      "|   2959|1427944|  7340|   4.5|   Fight Club (1999)|Action Crime Dram...| 4.228310618821569|1999|Action Crime Dram...|   Fight Club (1999)|Action|Crime|Dram...|\n",
      "|   1197| 576588|  7340|   4.5|Princess Bride, T...|Action Adventure ...| 4.129097007632781|1987|Action Adventure ...|Princess Bride, T...|Action|Adventure|...|\n",
      "|   7361| 203265|  7340|   4.5|Eternal Sunshine ...|Drama Romance Sci-Fi| 4.066350639578632|2004|Drama Romance Sci...|Eternal Sunshine ...|Drama|Romance|Sci-Fi|\n",
      "+-------+-------+------+------+--------------------+--------------------+------------------+----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.join(movies, on='movieId').filter('userId = 7340').sort('rating', ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+--------------------+--------------------+\n",
      "|movieId|userId|   rating|               title|              genres|\n",
      "+-------+------+---------+--------------------+--------------------+\n",
      "| 165575|  7340|5.0972724|In guerra per amo...|  (no genres listed)|\n",
      "|  99764|  7340| 4.679859|It's Such a Beaut...|Animation|Comedy|...|\n",
      "| 168716|  7340|4.6706047|A Gathering of Ca...|  (no genres listed)|\n",
      "| 150696|  7340| 4.443341|     Tomorrow (2015)|         Documentary|\n",
      "|   5912|  7340|4.3567977|Hit the Bank (Vab...|        Comedy|Crime|\n",
      "+-------+------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nrecoomendations.join(movies, on='movieId').filter('userId=7340').sort('rating', ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting model into a .pickl file for use in front-end\n",
    "nrecoomendations.toPandas().to_csv(\"collab_filter_recommendations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.toPandas().to_csv(\"collab_filter_movies.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
